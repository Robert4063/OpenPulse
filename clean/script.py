"""
Commentæ•°æ®æ¸…æ´—ä¸åˆ†ç»„è„šæœ¬
åŠŸèƒ½:
1. åªä¿ç•™æŒ‡å®šå­—æ®µ: id, body, user, created_at, updated_at, html_url, issue_url
2. æŒ‰issue_urlåˆ†ç»„,å°†åŒä¸€ä¸ªé—®é¢˜çš„æ‰€æœ‰commentsæ”¾åœ¨ä¸€èµ·
3. è¾“å‡ºåˆ°æ–°çš„ç›®å½•ç»“æ„
4. å¢å¼ºçš„é”™è¯¯å¤„ç†å’ŒJSONä¿®å¤èƒ½åŠ›
"""
import os
import json
import re
from typing import List, Dict, Optional
from collections import defaultdict

# é…ç½®
COMMENT_DIR = os.path.join("data", "comment")
OUTPUT_DIR = os.path.join("data", "comment_cleaned")
ERROR_DIR = os.path.join("data", "comment_errors")
BACKUP_SUFFIX = ".backup"

# éœ€è¦ä¿ç•™çš„å­—æ®µï¼ˆåŒ¹é…å®é™…JSONä¸­çš„å­—æ®µåï¼‰
KEEP_FIELDS = ["id", "body", "user", "created_at", "updated_at", "html_url", "issue_url"]

def clean_json_content(content: str) -> str:
    """
    æ¸…ç†JSONå†…å®¹ä¸­çš„æ§åˆ¶å­—ç¬¦å’Œæ ¼å¼é—®é¢˜
    
    Args:
        content: åŸå§‹JSONå­—ç¬¦ä¸²
        
    Returns:
        æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
    """
    # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œã€åˆ¶è¡¨ç¬¦ç­‰æ­£å¸¸å­—ç¬¦ï¼‰
    content = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]', '', content)
    
    # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
    # ç§»é™¤å¯èƒ½çš„UTF-8 BOM
    if content.startswith('\ufeff'):
        content = content[1:]
     
    return content.strip()

def try_parse_json(content: str, filepath: str) -> Optional[List[Dict]]:
    """
    å°è¯•å¤šç§æ–¹æ³•è§£æJSON
    
    Args:   
        content: JSONå­—ç¬¦ä¸²
        filepath: æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºé”™è¯¯æ—¥å¿—ï¼‰
        
    Returns:
        è§£æåçš„æ•°æ®åˆ—è¡¨ï¼Œå¤±è´¥è¿”å›None
    """
    filename = os.path.basename(filepath)
    
    # æ–¹æ³•1: ç›´æ¥è§£æ
    try:
        data = json.loads(content)
        if isinstance(data, list):
            return data
        else:
            print(f"  âš ï¸  {filename}: æ•°æ®ä¸æ˜¯æ•°ç»„æ ¼å¼")
            return None
    except json.JSONDecodeError as e:
        error_msg = str(e)
        
        # æ–¹æ³•2: å¦‚æœæ˜¯"Extra data"é”™è¯¯ï¼Œå°è¯•åªè¯»å–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONæ•°ç»„
        if "Extra data" in error_msg:
            print(f"  âš ï¸  {filename}: æ£€æµ‹åˆ°Extra dataé”™è¯¯ï¼Œå°è¯•ä¿®å¤...")
            try:
                # ä½¿ç”¨JSONDecoderé€ä¸ªè§£æ
                decoder = json.JSONDecoder()
                data, idx = decoder.raw_decode(content)
                if isinstance(data, list):
                    remaining = content[idx:].strip()
                    if remaining:
                        print(f"     è­¦å‘Š: æ–‡ä»¶æœ«å°¾æœ‰ {len(remaining)} å­—ç¬¦è¢«å¿½ç•¥")
                    return data
            except Exception as e2:
                print(f"     ä¿®å¤å¤±è´¥: {e2}")
        
        # æ–¹æ³•3: å°è¯•é€è¡Œè§£æï¼ˆå¯èƒ½æ˜¯JSONLæ ¼å¼ï¼‰
        if "Extra data" in error_msg or "Expecting" in error_msg:
            print(f"  âš ï¸  {filename}: å°è¯•ä½œä¸ºå¤šè¡ŒJSONè§£æ...")
            try:
                lines = content.strip().split('\n')
                all_data = []
                for i, line in enumerate(lines):
                    line = line.strip()
                    if line and (line.startswith('[') or line.startswith('{')):
                        try:
                            item = json.loads(line)
                            if isinstance(item, list):
                                all_data.extend(item)
                            elif isinstance(item, dict):
                                all_data.append(item)
                        except:
                            pass
                if all_data:
                    print(f"     æˆåŠŸè§£æ {len(all_data)} æ¡æ•°æ®")
                    return all_data
            except Exception as e3:
                print(f"     å¤šè¡Œè§£æå¤±è´¥: {e3}")
        
        # æ–¹æ³•4: å°è¯•ä¿®å¤æˆªæ–­çš„JSON
        print(f"  âš ï¸  {filename}: å°è¯•ä¿®å¤æˆªæ–­çš„JSON...")
        try:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
            last_complete = content.rfind('}')
            if last_complete > 0:
                # ç¡®ä¿ä»¥ ']' ç»“å°¾
                truncated = content[:last_complete+1]
                if not truncated.rstrip().endswith(']'):
                    truncated = truncated + ']'
                data = json.loads(truncated)
                if isinstance(data, list):
                    print(f"     ä¿®å¤æˆåŠŸï¼Œè§£æ {len(data)} æ¡æ•°æ®")
                    return data
        except Exception as e4:
            print(f"     ä¿®å¤å¤±è´¥: {e4}")
        
        print(f"  âŒ {filename}: æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥ - {error_msg}")
        return None

def clean_comment(comment: Dict) -> Dict:
    """
    æ¸…æ´—å•æ¡comment,åªä¿ç•™æŒ‡å®šå­—æ®µ
    
    Args:
        comment: åŸå§‹commentæ•°æ®
        
    Returns:
        æ¸…æ´—åçš„commentæ•°æ®
    """
    cleaned = {}
    for field in KEEP_FIELDS:
        if field in comment:
            cleaned[field] = comment[field]
        else:
            # å¦‚æœå­—æ®µä¸å­˜åœ¨,è®¾ä¸ºNone
            cleaned[field] = None
    
    return cleaned

def group_comments_by_issue(comments: List[Dict]) -> Dict[str, List[Dict]]:
    """
    æŒ‰issue_urlåˆ†ç»„comments
    
    Args:
        comments: commentåˆ—è¡¨
        
    Returns:
        ä»¥issue_urlä¸ºkeyçš„å­—å…¸,valueä¸ºè¯¥issueçš„æ‰€æœ‰comments
    """
    grouped = defaultdict(list)
    no_issue_count = 0
    
    for comment in comments:
        issue_url = comment.get("issue_url")
        if issue_url:
            grouped[issue_url].append(comment)
        else:
            # å¦‚æœæ²¡æœ‰issue_url,å½’å…¥"unknown"ç»„
            grouped["unknown"].append(comment)
            no_issue_count += 1
    
    if no_issue_count > 0:
        print(f"  âš ï¸  {no_issue_count} æ¡commentsæ²¡æœ‰issue_url")
    
    return dict(grouped)

def extract_issue_number(issue_url: str) -> str:
    """
    ä»issue_urlä¸­æå–issueç¼–å·
    ä¾‹å¦‚: "https://api.github.com/repos/owner/repo/issues/123" -> "issue_123"
    
    Args:
        issue_url: issueçš„URL
        
    Returns:
        issueç¼–å·å­—ç¬¦ä¸²
    """
    if not issue_url or issue_url == "unknown":
        return "unknown"
    
    try:
        # æå–URLæœ€åçš„æ•°å­—éƒ¨åˆ†
        parts = issue_url.rstrip('/').split('/')
        issue_num = parts[-1]
        return f"issue_{issue_num}"
    except:
        return "unknown"

def process_json_file(filepath: str) -> tuple[List[Dict], bool]:
    """
    å¤„ç†å•ä¸ªJSONæ–‡ä»¶,æ¸…æ´—å¹¶è¿”å›æ‰€æœ‰comments
    
    Args:
        filepath: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        (æ¸…æ´—åçš„commentsåˆ—è¡¨, æ˜¯å¦æˆåŠŸ)
    """
    filename = os.path.basename(filepath)
    
    try:
        # è¯»å–åŸå§‹æ•°æ®
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # æ¸…ç†å†…å®¹
        content = clean_json_content(content)
        
        # å°è¯•è§£æJSON
        data = try_parse_json(content, filepath)
        
        if data is None:
            return [], False
        
        # æ¸…æ´—æ¯æ¡comment
        cleaned_comments = [clean_comment(comment) for comment in data]
        
        print(f"  âœ“ {filename}: å¤„ç† {len(cleaned_comments)} æ¡comments")
        
        return cleaned_comments, True
        
    except Exception as e:
        print(f"  âŒ {filename}: å¤„ç†å¤±è´¥ - {e}")
        return [], False

def process_all_comments(directory: str) -> tuple[List[Dict], dict]:
    """
    å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶,åˆå¹¶æ‰€æœ‰comments
    
    Args:
        directory: ç›®å½•è·¯å¾„
        
    Returns:
        (æ‰€æœ‰æ¸…æ´—åçš„commentsåˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
    """
    if not os.path.exists(directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return [], {}
    
    all_comments = []
    stats = {
        "total_files": 0,
        "success_files": 0,
        "failed_files": 0,
        "failed_file_names": []
    }
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = [f for f in os.listdir(directory) 
                  if f.endswith('.json') and not f.endswith(BACKUP_SUFFIX)]
    
    print(f"\nğŸ“ å¤„ç†ç›®å½•: {directory}")
    print(f"   æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶\n")
    
    for filename in sorted(json_files):
        filepath = os.path.join(directory, filename)
        comments, success = process_json_file(filepath)
        
        stats["total_files"] += 1
        if success:
            stats["success_files"] += 1
            all_comments.extend(comments)
        else:
            stats["failed_files"] += 1
            stats["failed_file_names"].append(filename)
    
    return all_comments, stats

def save_grouped_comments(grouped_comments: Dict[str, List[Dict]], output_dir: str):
    """
    ä¿å­˜åˆ†ç»„åçš„commentsåˆ°æ–‡ä»¶
    
    Args:
        grouped_comments: æŒ‰issue_urlåˆ†ç»„çš„comments
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜åˆ†ç»„æ•°æ®åˆ°: {output_dir}\n")
    
    # ä¿å­˜æ¯ä¸ªissueçš„comments
    saved_count = 0
    for issue_url, comments in sorted(grouped_comments.items()):
        # ç”Ÿæˆæ–‡ä»¶å
        issue_id = extract_issue_number(issue_url)
        filename = f"{issue_id}.json"
        filepath = os.path.join(output_dir, filename)
        
        # æŒ‰created_atæ’åºcomments
        sorted_comments = sorted(
            comments, 
            key=lambda x: x.get("created_at") or ""
        )
        
        # æ„å»ºè¾“å‡ºæ•°æ®
        output_data = {
            "issue_url": issue_url,
            "comment_count": len(sorted_comments),
            "comments": sorted_comments
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        saved_count += 1
        if saved_count <= 10 or len(sorted_comments) > 100:
            print(f"  âœ“ {filename}: {len(sorted_comments)} æ¡comments")
    
    if saved_count > 10:
        print(f"  ... (çœç•¥ {saved_count - 10} ä¸ªæ–‡ä»¶)")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§¹ Commentæ•°æ®æ¸…æ´—ä¸åˆ†ç»„å·¥å…·")
    print("=" * 60)
    print(f"\nåŠŸèƒ½è¯´æ˜:")
    print(f"  1. åªä¿ç•™å­—æ®µ: {', '.join(KEEP_FIELDS)}")
    print(f"  2. æŒ‰issue_urlåˆ†ç»„comments")
    print(f"  3. è¾“å‡ºåˆ°: {OUTPUT_DIR}")
    print(f"  4. å¢å¼ºçš„JSONè§£æå’Œé”™è¯¯ä¿®å¤")
    
    # ç¡®è®¤æ‰§è¡Œ
    confirm = input("\nç¡®è®¤å¼€å§‹å¤„ç†? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return
    
    print("\n" + "=" * 60)
    
    # æ­¥éª¤1: è¯»å–å¹¶æ¸…æ´—æ‰€æœ‰comments
    print("\nğŸ“– æ­¥éª¤1: è¯»å–å¹¶æ¸…æ´—comments")
    all_comments, stats = process_all_comments(COMMENT_DIR)
    
    print(f"\næ–‡ä»¶å¤„ç†ç»Ÿè®¡:")
    print(f"  - æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  - æˆåŠŸ: {stats['success_files']}")
    print(f"  - å¤±è´¥: {stats['failed_files']}")
    
    if stats['failed_files'] > 0:
        print(f"\nå¤±è´¥çš„æ–‡ä»¶:")
        for name in stats['failed_file_names']:
            print(f"  - {name}")
    
    if not all_comments:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„commentsæ•°æ®")
        return
    
    print(f"\nâœ“ å…±è¯»å– {len(all_comments):,} æ¡comments")
    
    # æ­¥éª¤2: æŒ‰issue_urlåˆ†ç»„
    print("\nğŸ“Š æ­¥éª¤2: æŒ‰issue_urlåˆ†ç»„")
    grouped_comments = group_comments_by_issue(all_comments)
    
    print(f"âœ“ åˆ†ç»„å®Œæˆ: {len(grouped_comments)} ä¸ªä¸åŒçš„issues")
    
    # æ˜¾ç¤ºåˆ†ç»„ç»Ÿè®¡
    issue_comment_counts = [(url, len(comments)) for url, comments in grouped_comments.items()]
    issue_comment_counts.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nå‰10ä¸ªæœ€å¤šcommentsçš„issues:")
    for i, (url, count) in enumerate(issue_comment_counts[:10], 1):
        issue_id = extract_issue_number(url)
        print(f"  {i}. {issue_id}: {count} æ¡comments")
    
    # æ­¥éª¤3: ä¿å­˜åˆ†ç»„åçš„æ•°æ®
    print(f"\nğŸ’¾ æ­¥éª¤3: ä¿å­˜åˆ†ç»„æ•°æ®")
    save_grouped_comments(grouped_comments, OUTPUT_DIR)
    
    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
    print("=" * 60)
    print(f"\næ€»è®¡:")
    print(f"  - å¤„ç†æ–‡ä»¶: {stats['success_files']}/{stats['total_files']}")
    print(f"  - å¤„ç†comments: {len(all_comments):,} æ¡")
    print(f"  - åˆ†ç»„issues: {len(grouped_comments)} ä¸ª")
    print(f"  - è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"  - å¹³å‡æ¯ä¸ªissue: {len(all_comments)/len(grouped_comments):.1f} æ¡comments")
    
    print("\n" + "=" * 60)
    print("âœ… å¤„ç†å®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    main()