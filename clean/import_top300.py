"""
å°† top300_2022_2023.csv æ–‡ä»¶å¯¼å…¥åˆ° Docker MySQL å®¹å™¨ (openpulse_mysql) çš„ github_data æ•°æ®åº“ä¸­

ä½¿ç”¨æ–¹æ³•:
    python import_top300.py

å¯é€‰å‚æ•°:
    --mode replace|append|fail  å¯¼å…¥æ¨¡å¼ï¼ˆé»˜è®¤: replaceï¼‰
    --chunksize N               æ¯æ¬¡è¯»å–çš„è¡Œæ•°ï¼ˆé»˜è®¤: 50000ï¼‰
"""

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.types import BigInteger, Text, Integer
from sqlalchemy.dialects.mysql import LONGTEXT
import pymysql
import argparse
import os
import sys
import shutil
import subprocess
from datetime import datetime

# ç¦ç”¨è¾“å‡ºç¼“å†²
sys.stdout.reconfigure(line_buffering=True)

# ====== ç£ç›˜ç©ºé—´ç›‘æ§é…ç½® ======
MIN_DISK_SPACE_GB = 10  # æœ€å°å‰©ä½™ç©ºé—´ï¼ˆGBï¼‰
DISK_TO_MONITOR = "C:\\"  # ç›‘æ§çš„ç£ç›˜

def check_disk_space():
    """æ£€æŸ¥ç£ç›˜å‰©ä½™ç©ºé—´ï¼Œè¿”å›å‰©ä½™ç©ºé—´ï¼ˆGBï¼‰"""
    total, used, free = shutil.disk_usage(DISK_TO_MONITOR)
    free_gb = free / (1024 ** 3)  # è½¬æ¢ä¸º GB
    return free_gb

def is_disk_space_low():
    """æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦ä½äºé˜ˆå€¼"""
    free_gb = check_disk_space()
    return free_gb < MIN_DISK_SPACE_GB, free_gb

# ====== 1. æ•°æ®åº“é…ç½® ======
DB_USER = 'root'
DB_PASSWORD = 'root'  # Docker MySQL å®¹å™¨çš„ root å¯†ç 
DB_HOST = '127.0.0.1'
DB_PORT = 3306
DB_NAME = 'openrankdata'  # ç›®æ ‡æ•°æ®åº“åç§°

# ====== 2. CSV æ–‡ä»¶é…ç½® ======
CSV_FILE_PATH = r'D:\openrankdata\top300_20_23\top300_2022_2023.csv'
TARGET_TABLE = 'top300_2022_2023'  # ç›®æ ‡è¡¨å

def test_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥å¹¶åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    try:
        # å…ˆå°è¯•è¿æ¥åˆ° MySQL æœåŠ¡å™¨ï¼ˆä¸æŒ‡å®šæ•°æ®åº“ï¼‰
        test_conn = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD
        )
        print("âœ… MySQL æœåŠ¡å™¨è¿æ¥æˆåŠŸï¼")
        
        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        cursor = test_conn.cursor()
        cursor.execute("SHOW DATABASES LIKE %s", (DB_NAME,))
        result = cursor.fetchone()
        
        if result:
            print(f"âœ… æ•°æ®åº“ '{DB_NAME}' å·²å­˜åœ¨")
        else:
            print(f"âš ï¸  æ•°æ®åº“ '{DB_NAME}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            cursor.execute(f"CREATE DATABASE IF NOT EXISTS `{DB_NAME}` CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci")
            print(f"âœ… æ•°æ®åº“ '{DB_NAME}' åˆ›å»ºæˆåŠŸ")
        
        cursor.close()
        test_conn.close()
        return True
        
    except pymysql.err.OperationalError as e:
        if e.args[0] == 2003:
            print("âŒ æ— æ³•è¿æ¥åˆ° MySQL æœåŠ¡å™¨ï¼")
            print("\nå¯èƒ½çš„åŸå› ï¼š")
            print("1. Docker MySQL å®¹å™¨ (openpulse_mysql) æœªå¯åŠ¨")
            print("   - æ£€æŸ¥å®¹å™¨çŠ¶æ€: docker ps -a")
            print("   - å¯åŠ¨å®¹å™¨: docker start openpulse_mysql")
            print("2. MySQL é…ç½®çš„ç«¯å£ä¸æ˜¯ 3306")
            print("3. ç«¯å£æ˜ å°„ä¸æ­£ç¡®ï¼Œæ£€æŸ¥ Docker ç«¯å£æ˜ å°„")
            print(f"\nå½“å‰é…ç½®: {DB_HOST}:{DB_PORT}")
            print(f"æ•°æ®åº“: {DB_NAME}, ç”¨æˆ·: {DB_USER}")
        else:
            print(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return False

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å¯¼å…¥ top300_2022_2023.csv åˆ° MySQL æ•°æ®åº“')
    parser.add_argument('--mode', choices=['replace', 'append', 'fail'], 
                       default='replace', 
                       help='å¯¼å…¥æ¨¡å¼ï¼šreplace/append/fail (é»˜è®¤: replace)')
    parser.add_argument('--chunksize', type=int, default=50000,
                       help='æ¯æ¬¡è¯»å–çš„è¡Œæ•°ï¼ˆé»˜è®¤: 50000ï¼‰')
    parser.add_argument('--after-script', type=str, default=None,
                       help='å¯¼å…¥å®Œæˆåè¦è¿è¡Œçš„è„šæœ¬è·¯å¾„ï¼ˆä¾‹å¦‚ï¼špython script.pyï¼‰')
    return parser.parse_args()

def get_dtype_mapping():
    """å®šä¹‰ SQLAlchemy æ•°æ®ç±»å‹æ˜ å°„ï¼Œé¿å…æ•°æ®æˆªæ–­é—®é¢˜"""
    # æ‰€æœ‰æ–‡æœ¬å­—æ®µä½¿ç”¨ TEXT ç±»å‹ï¼Œå¯èƒ½å¾ˆé•¿çš„å†…å®¹ä½¿ç”¨ LONGTEXT ç±»å‹ï¼Œæ•°å€¼å­—æ®µä½¿ç”¨ BigInteger
    dtype_mapping = {
        'id': BigInteger(),
        'type': Text(),
        'action': Text(),
        'actor_id': BigInteger(),
        'actor_login': Text(),
        'repo_id': BigInteger(),
        'repo_name': Text(),
        'org_id': BigInteger(),
        'org_login': Text(),
        'created_at': Text(),
        'issue_id': BigInteger(),
        'issue_number': BigInteger(),
        'issue_title': LONGTEXT(),  # æ ‡é¢˜å¯èƒ½å¾ˆé•¿
        'body': LONGTEXT(),  # Issue/PR body å¯èƒ½éå¸¸é•¿
        'issue_labels_name': LONGTEXT(),  # JSON æ•°ç»„å¯èƒ½å¾ˆé•¿
        'issue_labels_color': LONGTEXT(),
        'issue_labels_default': LONGTEXT(),
        'issue_labels_description': LONGTEXT(),
        'issue_author_id': BigInteger(),
        'issue_author_login': Text(),
        'issue_author_type': Text(),
        'issue_author_association': Text(),
        'issue_assignee_id': BigInteger(),
        'issue_assignee_login': Text(),
        'issue_assignees_login': LONGTEXT(),  # JSON æ•°ç»„
        'issue_assignees_id': LONGTEXT(),  # JSON æ•°ç»„
        'issue_created_at': Text(),
        'issue_updated_at': Text(),
        'issue_comments': BigInteger(),
        'issue_closed_at': Text(),
        'issue_comment_id': BigInteger(),
        'issue_comment_created_at': Text(),
        'issue_comment_updated_at': Text(),
        'issue_comment_author_association': Text(),
        'issue_comment_author_id': BigInteger(),
        'issue_comment_author_login': Text(),
        'issue_comment_author_type': Text(),
        'pull_commits': BigInteger(),
        'pull_additions': BigInteger(),
        'pull_deletions': BigInteger(),
        'pull_changed_files': BigInteger(),
        'pull_merged': Integer(),
        'pull_merge_commit_sha': Text(),
        'pull_merged_at': Text(),
        'pull_merged_by_id': BigInteger(),
        'pull_merged_by_login': Text(),
        'pull_merged_by_type': Text(),
        'pull_requested_reviewer_id': BigInteger(),
        'pull_requested_reviewer_login': Text(),
        'pull_requested_reviewer_type': Text(),
        'pull_review_comments': BigInteger(),
        'repo_description': LONGTEXT(),  # é¡¹ç›®æè¿°å¯èƒ½å¾ˆé•¿
        'repo_size': BigInteger(),
        'repo_stargazers_count': BigInteger(),
        'repo_forks_count': BigInteger(),
        'repo_language': Text(),
        'repo_has_issues': Integer(),
        'repo_has_projects': Integer(),
        'repo_has_downloads': Integer(),
        'repo_has_wiki': Integer(),
        'repo_has_pages': Integer(),
        'repo_license': Text(),
        'repo_default_branch': Text(),
        'repo_created_at': Text(),
        'repo_updated_at': Text(),
        'repo_pushed_at': Text(),
        'pull_review_state': Text(),
        'pull_review_author_association': Text(),
        'pull_review_id': BigInteger(),
        'pull_review_comment_id': BigInteger(),
        'pull_review_comment_path': LONGTEXT(),  # æ–‡ä»¶è·¯å¾„å¯èƒ½å¾ˆé•¿
        'pull_review_comment_position': BigInteger(),
        'pull_review_comment_author_id': BigInteger(),
        'pull_review_comment_author_login': Text(),
        'pull_review_comment_author_type': Text(),
        'pull_review_comment_author_association': Text(),
        'pull_review_comment_created_at': Text(),
        'pull_review_comment_updated_at': Text(),
        'push_id': BigInteger(),
        'push_size': BigInteger(),
        'push_distinct_size': BigInteger(),
        'push_ref': LONGTEXT(),
        'push_head': Text(),
        'push_commits_name': LONGTEXT(),  # JSON æ•°ç»„
        'push_commits_email': LONGTEXT(),  # JSON æ•°ç»„
        'push_commits_message': LONGTEXT(),  # æäº¤æ¶ˆæ¯å¯èƒ½å¾ˆé•¿ï¼ŒJSON æ•°ç»„
        'fork_forkee_id': BigInteger(),
        'fork_forkee_full_name': Text(),
        'fork_forkee_owner_id': BigInteger(),
        'fork_forkee_owner_login': Text(),
        'fork_forkee_owner_type': Text(),
        'delete_ref': LONGTEXT(),
        'delete_ref_type': Text(),
        'delete_pusher_type': Text(),
        'create_ref': LONGTEXT(),
        'create_ref_type': Text(),
        'create_master_branch': Text(),
        'create_description': LONGTEXT(),
        'create_pusher_type': Text(),
        'gollum_pages_page_name': LONGTEXT(),  # JSON æ•°ç»„
        'gollum_pages_title': LONGTEXT(),  # JSON æ•°ç»„
        'gollum_pages_action': LONGTEXT(),  # JSON æ•°ç»„
        'member_id': BigInteger(),
        'member_login': Text(),
        'member_type': Text(),
        'release_id': BigInteger(),
        'release_tag_name': Text(),
        'release_target_commitish': Text(),
        'release_name': LONGTEXT(),
        'release_draft': Integer(),
        'release_author_id': BigInteger(),
        'release_author_login': Text(),
        'release_author_type': Text(),
        'release_prerelease': Integer(),
        'release_created_at': Text(),
        'release_published_at': Text(),
        'release_body': LONGTEXT(),  # Release notes å¯èƒ½éå¸¸é•¿
        'release_assets_name': LONGTEXT(),  # JSON æ•°ç»„
        'release_assets_uploader_login': LONGTEXT(),
        'release_assets_uploader_id': LONGTEXT(),
        'release_assets_content_type': LONGTEXT(),
        'release_assets_state': LONGTEXT(),
        'release_assets_size': LONGTEXT(),
        'release_assets_download_count': LONGTEXT(),
        'commit_comment_id': BigInteger(),
        'commit_comment_author_id': BigInteger(),
        'commit_comment_author_login': Text(),
        'commit_comment_author_type': Text(),
        'commit_comment_author_association': Text(),
        'commit_comment_path': LONGTEXT(),
        'commit_comment_position': Text(),
        'commit_comment_line': Text(),
        'commit_comment_created_at': Text(),
        'commit_comment_updated_at': Text(),
    }
    return dtype_mapping

def count_csv_rows(file_path):
    """å¿«é€Ÿè®¡ç®— CSV æ–‡ä»¶çš„æ€»è¡Œæ•°ï¼ˆä¸è¯»å…¥å†…å­˜ï¼‰"""
    print("ğŸ“Š æ­£åœ¨è®¡ç®— CSV æ–‡ä»¶æ€»è¡Œæ•°...")
    count = 0
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        for _ in f:
            count += 1
    return count - 1  # å‡å»æ ‡é¢˜è¡Œ

def import_csv_to_mysql(engine, import_mode, chunksize):
    """å°† CSV æ–‡ä»¶åˆ†å—å¯¼å…¥åˆ° MySQL æ•°æ®åº“"""
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CSV_FILE_PATH):
        print(f"âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨: {CSV_FILE_PATH}")
        return False
    
    # è·å–æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(CSV_FILE_PATH) / (1024 * 1024)  # MB
    print(f"ğŸ“‚ CSV æ–‡ä»¶: {CSV_FILE_PATH}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    # è®¡ç®—æ€»è¡Œæ•°
    total_csv_rows = count_csv_rows(CSV_FILE_PATH)
    total_chunks = (total_csv_rows + chunksize - 1) // chunksize  # å‘ä¸Šå–æ•´
    print(f"ğŸ“Š CSV æ€»è¡Œæ•°: {total_csv_rows:,} è¡Œ")
    print(f"ğŸ“Š é¢„è®¡åˆ†å—æ•°: {total_chunks} å—")
    print(f"ğŸ“‹ ç›®æ ‡è¡¨: {TARGET_TABLE}")
    print(f"ğŸ“‹ å¯¼å…¥æ¨¡å¼: {import_mode}")
    print(f"ğŸ“‹ åˆ†å—å¤§å°: {chunksize} è¡Œ")
    print()
    
    # è·å–æ•°æ®ç±»å‹æ˜ å°„
    dtype_mapping = get_dtype_mapping()
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
    table_exists = False
    try:
        with engine.connect() as conn:
            result = conn.execute(text(f"SHOW TABLES LIKE '{TARGET_TABLE}'"))
            table_exists = result.fetchone() is not None
    except Exception:
        table_exists = False
    
    if table_exists and import_mode == 'replace':
        print(f"âš ï¸  è­¦å‘Š: è¡¨ '{TARGET_TABLE}' å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–ï¼")
    elif table_exists and import_mode == 'append':
        print(f"â„¹ï¸  è¡¨ '{TARGET_TABLE}' å·²å­˜åœ¨ï¼Œå°†è¿½åŠ æ•°æ®")
    elif table_exists and import_mode == 'fail':
        print(f"âŒ è¡¨ '{TARGET_TABLE}' å·²å­˜åœ¨ï¼Œå¯¼å…¥æ¨¡å¼ä¸º 'fail'ï¼Œåœæ­¢å¯¼å…¥")
        return False
    elif not table_exists:
        print(f"â„¹ï¸  è¡¨ '{TARGET_TABLE}' ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°è¡¨")
    
    print()
    print("ğŸš€ å¼€å§‹å¯¼å…¥æ•°æ®...")
    start_time = datetime.now()
    
    try:
        total_rows = 0
        chunk_count = 0
        stopped_due_to_disk = False
        
        # ä½¿ç”¨ chunksize åˆ†å—è¯»å–å¤§å‹ CSV æ–‡ä»¶
        for chunk in pd.read_csv(CSV_FILE_PATH, chunksize=chunksize, encoding='utf-8', low_memory=False):
            # æ¯æ¬¡å¯¼å…¥å‰æ£€æŸ¥ç£ç›˜ç©ºé—´
            is_low, free_gb = is_disk_space_low()
            if is_low:
                print()
                print("=" * 60)
                print(f"âš ï¸  è­¦å‘Š: Cç›˜å‰©ä½™ç©ºé—´ä¸è¶³ï¼")
                print(f"   å½“å‰å‰©ä½™: {free_gb:.2f} GB")
                print(f"   æœ€å°è¦æ±‚: {MIN_DISK_SPACE_GB} GB")
                print(f"   å·²å¯¼å…¥: {total_rows:,} è¡Œ")
                print("   å¯¼å…¥å·²è‡ªåŠ¨åœæ­¢ï¼Œè¯·æ¸…ç†ç£ç›˜ç©ºé—´åé‡æ–°è¿è¡Œï¼ˆä½¿ç”¨ --mode appendï¼‰")
                print("=" * 60)
                stopped_due_to_disk = True
                break
            
            chunk_count += 1
            rows_in_chunk = len(chunk)
            total_rows += rows_in_chunk
            
            # å¯¹äºç¬¬ä¸€ä¸ªå—ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å¼ï¼›åç»­å—ä½¿ç”¨ append æ¨¡å¼
            if chunk_count == 1:
                chunk.to_sql(name=TARGET_TABLE, con=engine, if_exists=import_mode, index=False, dtype=dtype_mapping)
            else:
                chunk.to_sql(name=TARGET_TABLE, con=engine, if_exists='append', index=False)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            progress = (total_rows / total_csv_rows) * 100
            remaining_rows = total_csv_rows - total_rows
            speed = total_rows / elapsed if elapsed > 0 else 0
            eta_seconds = remaining_rows / speed if speed > 0 else 0
            eta_minutes = eta_seconds / 60
            free_gb = check_disk_space()
            
            # è¿›åº¦æ¡
            bar_length = 30
            filled_length = int(bar_length * progress / 100)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            
            print(f"   âœ… å— {chunk_count}/{total_chunks}: [{bar}] {progress:.1f}% | {total_rows:,}/{total_csv_rows:,} è¡Œ | è€—æ—¶: {elapsed:.0f}s | å‰©ä½™: {eta_minutes:.1f}åˆ†é’Ÿ | Cç›˜: {free_gb:.1f}GB")
        
        if stopped_due_to_disk:
            return False
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print()
        print("=" * 50)
        print(f"âœ… å¯¼å…¥å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è¡Œæ•°: {total_rows}")
        print(f"ğŸ“Š å—æ•°é‡: {chunk_count}")
        print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ“Š å¹³å‡é€Ÿåº¦: {total_rows/duration:.0f} è¡Œ/ç§’")
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def run_after_script(script_command):
    """å¯¼å…¥å®Œæˆåè¿è¡ŒæŒ‡å®šçš„è„šæœ¬"""
    if not script_command:
        return
    
    print()
    print("=" * 50)
    print(f"ğŸš€ æ­£åœ¨è¿è¡Œåç»­è„šæœ¬: {script_command}")
    print("=" * 50)
    
    try:
        # ä½¿ç”¨ shell=True æ¥æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(script_command, shell=True, cwd=os.path.dirname(os.path.abspath(__file__)))
        if result.returncode == 0:
            print(f"âœ… åç»­è„šæœ¬æ‰§è¡ŒæˆåŠŸï¼")
        else:
            print(f"âš ï¸  åç»­è„šæœ¬æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç : {result.returncode}")
    except Exception as e:
        print(f"âŒ åç»­è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    print("=" * 50)
    print("ğŸ“¦ top300_2022_2023.csv å¯¼å…¥å·¥å…·")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    print()
    
    # æ˜¾ç¤ºç£ç›˜ç©ºé—´ç›‘æ§ä¿¡æ¯
    free_gb = check_disk_space()
    print(f"ğŸ’¾ Cç›˜å½“å‰å‰©ä½™ç©ºé—´: {free_gb:.2f} GB")
    print(f"ğŸ’¾ æœ€å°ç©ºé—´é˜ˆå€¼: {MIN_DISK_SPACE_GB} GB")
    if args.after_script:
        print(f"ğŸ“œ å®Œæˆåå°†è¿è¡Œ: {args.after_script}")
    print()
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not test_connection():
        print("\nè¯·å…ˆè§£å†³æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œç„¶åé‡æ–°è¿è¡Œè„šæœ¬ã€‚")
        return
    
    # å»ºç«‹ SQLAlchemy è¿æ¥
    connection_str = f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4"
    engine = create_engine(connection_str)
    
    # æµ‹è¯• SQLAlchemy è¿æ¥
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print("âœ… SQLAlchemy è¿æ¥æµ‹è¯•æˆåŠŸï¼")
        print()
    except Exception as e:
        print(f"âŒ SQLAlchemy è¿æ¥å¤±è´¥: {e}")
        return
    
    # æ‰§è¡Œå¯¼å…¥
    success = import_csv_to_mysql(engine, args.mode, args.chunksize)
    
    # å¦‚æœå¯¼å…¥æˆåŠŸä¸”æŒ‡å®šäº†åç»­è„šæœ¬ï¼Œåˆ™è¿è¡Œ
    if success and args.after_script:
        run_after_script(args.after_script)

if __name__ == '__main__':
    main()
