"""
å°† data æ–‡ä»¶å¤¹ä¸­çš„ comment_cleaned, fork, issue, star æ•°æ®å¯¼å…¥åˆ° MySQL æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python import_data_folder.py                    # å¯¼å…¥æ‰€æœ‰ç±»å‹
    python import_data_folder.py --type star        # åªå¯¼å…¥ star æ•°æ®
    python import_data_folder.py --type fork        # åªå¯¼å…¥ fork æ•°æ®
    python import_data_folder.py --type issue       # åªå¯¼å…¥ issue æ•°æ®
    python import_data_folder.py --type comment     # åªå¯¼å…¥ comment æ•°æ®
    python import_data_folder.py --mode append      # è¿½åŠ æ¨¡å¼

ç›®æ ‡æ•°æ®åº“:
    - Dockerå®¹å™¨: openpulse_data
    - æ•°æ®åº“: openrankdata
    - ç«¯å£: 3306
    - å¯†ç : root
"""

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.types import BigInteger, Text, Integer
from sqlalchemy.dialects.mysql import LONGTEXT
import pymysql
import argparse
import os
import sys
import json
import shutil
from datetime import datetime

# ç¦ç”¨è¾“å‡ºç¼“å†²
sys.stdout.reconfigure(line_buffering=True)

# ====== ç£ç›˜ç©ºé—´ç›‘æ§é…ç½® ======
MIN_DISK_SPACE_GB = 10  # æœ€å°å‰©ä½™ç©ºé—´ï¼ˆGBï¼‰
DISK_TO_MONITOR = "C:\\"  # ç›‘æ§çš„ç£ç›˜

# ====== æ•°æ®åº“é…ç½® ======
DB_USER = 'root'
DB_PASSWORD = 'root'
DB_HOST = '127.0.0.1'
DB_PORT = 3306
DB_NAME = 'openrankdata'

# ====== æ•°æ®æ–‡ä»¶å¤¹é…ç½® ======
DATA_FOLDER = r'D:\openrankdata\data'
DATA_TYPES = {
    'star': {
        'folder': 'star',
        'table': 'stars',
        'file_suffix': '_stars.json'
    },
    'fork': {
        'folder': 'fork',
        'table': 'forks',
        'file_suffix': '_forks.json'
    },
    'issue': {
        'folder': 'issue',
        'table': 'issues',
        'file_suffix': '.json'
    },
    'comment': {
        'folder': 'comment_cleaned',
        'table': 'comments',
        'file_suffix': '.json'
    }
}

def check_disk_space():
    """æ£€æŸ¥ç£ç›˜å‰©ä½™ç©ºé—´ï¼Œè¿”å›å‰©ä½™ç©ºé—´ï¼ˆGBï¼‰"""
    total, used, free = shutil.disk_usage(DISK_TO_MONITOR)
    free_gb = free / (1024 ** 3)
    return free_gb

def is_disk_space_low():
    """æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦ä½äºé˜ˆå€¼"""
    free_gb = check_disk_space()
    return free_gb < MIN_DISK_SPACE_GB, free_gb

def test_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥å¹¶åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    try:
        test_conn = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD
        )
        print("âœ… MySQL æœåŠ¡å™¨è¿æ¥æˆåŠŸï¼")
        
        cursor = test_conn.cursor()
        cursor.execute("SHOW DATABASES LIKE %s", (DB_NAME,))
        result = cursor.fetchone()
        
        if result:
            print(f"âœ… æ•°æ®åº“ '{DB_NAME}' å·²å­˜åœ¨")
        else:
            print(f"âš ï¸  æ•°æ®åº“ '{DB_NAME}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            cursor.execute(f"CREATE DATABASE IF NOT EXISTS `{DB_NAME}` CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci")
            print(f"âœ… æ•°æ®åº“ '{DB_NAME}' åˆ›å»ºæˆåŠŸ")
        
        cursor.close()
        test_conn.close()
        return True
        
    except pymysql.err.OperationalError as e:
        if e.args[0] == 2003:
            print("âŒ æ— æ³•è¿æ¥åˆ° MySQL æœåŠ¡å™¨ï¼")
            print("\nå¯èƒ½çš„åŸå› ï¼š")
            print("1. Docker MySQL å®¹å™¨ (openpulse_data) æœªå¯åŠ¨")
            print("   - æ£€æŸ¥å®¹å™¨çŠ¶æ€: docker ps -a")
            print("   - å¯åŠ¨å®¹å™¨: docker start openpulse_data")
            print(f"\nå½“å‰é…ç½®: {DB_HOST}:{DB_PORT}")
            print(f"æ•°æ®åº“: {DB_NAME}, ç”¨æˆ·: {DB_USER}")
        else:
            print(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return False

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å¯¼å…¥ data æ–‡ä»¶å¤¹æ•°æ®åˆ° MySQL æ•°æ®åº“')
    parser.add_argument('--type', choices=['star', 'fork', 'issue', 'comment', 'all'], 
                       default='all', 
                       help='æ•°æ®ç±»å‹ï¼šstar/fork/issue/comment/all (é»˜è®¤: all)')
    parser.add_argument('--mode', choices=['replace', 'append', 'fail'], 
                       default='replace', 
                       help='å¯¼å…¥æ¨¡å¼ï¼šreplace/append/fail (é»˜è®¤: replace)')
    return parser.parse_args()

def process_star_fork_file(file_path, data_type):
    """å¤„ç† star æˆ– fork ç±»å‹çš„ JSON æ–‡ä»¶ï¼Œå±•å¼€ä¸ºå¤šè¡Œæ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    project = data.get('project', os.path.basename(file_path).replace('_stars.json', '').replace('_forks.json', '').replace('_', '/'))
    start_date = data.get('start_date', '')
    end_date = data.get('end_date', '')
    
    if data_type == 'star':
        total_count = data.get('total_stargazers', 0)
        daily_data = data.get('daily_stars', {})
        count_field = 'stars_count'
        total_field = 'total_stargazers'
    else:  # fork
        total_count = data.get('total_forks', 0)
        daily_data = data.get('daily_forks', {})
        count_field = 'forks_count'
        total_field = 'total_forks'
    
    rows = []
    for date, count in daily_data.items():
        rows.append({
            'project': project,
            'date': date,
            count_field: count,
            total_field: total_count
        })
    
    return pd.DataFrame(rows)

def process_issue_file(file_path):
    """å¤„ç† issue ç±»å‹çš„ JSON æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ä»æ–‡ä»¶åæå–é¡¹ç›®å
    file_name = os.path.basename(file_path)
    project = file_name.replace('.json', '').replace('_', '/')
    
    rows = []
    for issue in data:
        rows.append({
            'project': project,
            'title': issue.get('title', ''),
            'body': issue.get('body', ''),
            'state': issue.get('state', ''),
            'number': issue.get('number', 0),
            'created_at': issue.get('created_at', ''),
            'closed_at': issue.get('closed_at', ''),
            'labels': json.dumps(issue.get('labels', []), ensure_ascii=False),
            'author_association': issue.get('author_association', ''),
            'user': issue.get('user', ''),
            'html_url': issue.get('html_url', '')
        })
    
    return pd.DataFrame(rows)

def extract_username(user_field):
    """ä» user å­—æ®µæå–ç”¨æˆ·åï¼Œuser å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸"""
    if user_field is None:
        return ''
    if isinstance(user_field, str):
        return user_field
    if isinstance(user_field, dict):
        return user_field.get('login', '')
    return str(user_field)

def process_comment_file(file_path):
    """å¤„ç† comment_cleaned ç±»å‹çš„ JSON æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ä»æ–‡ä»¶åæå–é¡¹ç›®å
    file_name = os.path.basename(file_path)
    project = file_name.replace('.json', '').replace('_', '/', 1)
    
    source_file = data.get('source_file', '')
    total_comments = data.get('total_comments', 0)
    
    rows = []
    for issue_data in data.get('issues', []):
        issue_url = issue_data.get('issue_url', '')
        for comment in issue_data.get('comments', []):
            rows.append({
                'project': project,
                'issue_url': issue_url,
                'comment_id': comment.get('id', 0),
                'body': comment.get('body', ''),
                'user': extract_username(comment.get('user')),
                'created_at': comment.get('created_at', ''),
                'updated_at': comment.get('updated_at', ''),
                'html_url': comment.get('html_url', '')
            })
    
    return pd.DataFrame(rows)

def get_dtype_mapping(data_type):
    """è·å–ä¸åŒæ•°æ®ç±»å‹çš„ SQLAlchemy ç±»å‹æ˜ å°„"""
    if data_type == 'star':
        return {
            'project': Text(),
            'date': Text(),
            'stars_count': Integer(),
            'total_stargazers': BigInteger()
        }
    elif data_type == 'fork':
        return {
            'project': Text(),
            'date': Text(),
            'forks_count': Integer(),
            'total_forks': BigInteger()
        }
    elif data_type == 'issue':
        return {
            'project': Text(),
            'title': LONGTEXT(),
            'body': LONGTEXT(),
            'state': Text(),
            'number': BigInteger(),
            'created_at': Text(),
            'closed_at': Text(),
            'labels': LONGTEXT(),
            'author_association': Text(),
            'user': Text(),
            'html_url': Text()
        }
    elif data_type == 'comment':
        return {
            'project': Text(),
            'issue_url': Text(),
            'comment_id': BigInteger(),
            'body': LONGTEXT(),
            'user': Text(),
            'created_at': Text(),
            'updated_at': Text(),
            'html_url': Text()
        }
    return {}

def import_data_type(engine, data_type, import_mode):
    """å¯¼å…¥æŒ‡å®šç±»å‹çš„æ•°æ®"""
    config = DATA_TYPES[data_type]
    folder_path = os.path.join(DATA_FOLDER, config['folder'])
    table_name = config['table']
    
    if not os.path.exists(folder_path):
        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        return False
    
    # è·å–æ‰€æœ‰ JSON æ–‡ä»¶
    files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
    
    if not files:
        print(f"âŒ æ–‡ä»¶å¤¹ {folder_path} ä¸­æ²¡æœ‰ JSON æ–‡ä»¶")
        return False
    
    print(f"\nğŸ“‚ æ­£åœ¨å¯¼å…¥ {data_type} æ•°æ®...")
    print(f"   æ–‡ä»¶å¤¹: {folder_path}")
    print(f"   æ–‡ä»¶æ•°: {len(files)}")
    print(f"   ç›®æ ‡è¡¨: {table_name}")
    print(f"   å¯¼å…¥æ¨¡å¼: {import_mode}")
    
    dtype_mapping = get_dtype_mapping(data_type)
    start_time = datetime.now()
    total_rows = 0
    processed_files = 0
    all_data = []
    
    for i, filename in enumerate(files):
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        is_low, free_gb = is_disk_space_low()
        if is_low:
            print(f"\nâš ï¸  Cç›˜ç©ºé—´ä¸è¶³ ({free_gb:.2f}GB < {MIN_DISK_SPACE_GB}GB)ï¼Œåœæ­¢å¯¼å…¥")
            break
        
        file_path = os.path.join(folder_path, filename)
        
        try:
            # æ ¹æ®æ•°æ®ç±»å‹å¤„ç†æ–‡ä»¶
            if data_type in ['star', 'fork']:
                df = process_star_fork_file(file_path, data_type)
            elif data_type == 'issue':
                df = process_issue_file(file_path)
            elif data_type == 'comment':
                df = process_comment_file(file_path)
            else:
                continue
            
            if df.empty:
                continue
            
            all_data.append(df)
            total_rows += len(df)
            processed_files += 1
            
            # è¿›åº¦æ˜¾ç¤º
            progress = ((i + 1) / len(files)) * 100
            elapsed = (datetime.now() - start_time).total_seconds()
            print(f"   [{i+1}/{len(files)}] {progress:.1f}% | {filename} | {len(df)} è¡Œ | Cç›˜: {check_disk_space():.1f}GB")
            
        except Exception as e:
            print(f"   âŒ å¤„ç† {filename} å¤±è´¥: {e}")
            continue
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®å¹¶å†™å…¥æ•°æ®åº“
    if all_data:
        print(f"\n   ğŸ“Š åˆå¹¶ {len(all_data)} ä¸ªæ–‡ä»¶çš„æ•°æ®...")
        combined_df = pd.concat(all_data, ignore_index=True)
        
        print(f"   ğŸ“Š å†™å…¥æ•°æ®åº“ ({total_rows:,} è¡Œ)...")
        combined_df.to_sql(name=table_name, con=engine, if_exists=import_mode, index=False, dtype=dtype_mapping)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"   âœ… {data_type} å¯¼å…¥å®Œæˆï¼")
        print(f"      æ–‡ä»¶æ•°: {processed_files}")
        print(f"      æ€»è¡Œæ•°: {total_rows:,}")
        print(f"      è€—æ—¶: {elapsed:.1f} ç§’")
        return True
    else:
        print(f"   âš ï¸  æ²¡æœ‰æ•°æ®å¯å¯¼å…¥")
        return False

def main():
    args = parse_args()
    
    print("=" * 60)
    print("ğŸ“¦ Data æ–‡ä»¶å¤¹å¯¼å…¥å·¥å…·")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"ğŸ’¾ Cç›˜å‰©ä½™ç©ºé—´: {check_disk_space():.2f} GB")
    print(f"ğŸ’¾ æœ€å°ç©ºé—´é˜ˆå€¼: {MIN_DISK_SPACE_GB} GB")
    print(f"ğŸ“‹ æ•°æ®ç±»å‹: {args.type}")
    print(f"ğŸ“‹ å¯¼å…¥æ¨¡å¼: {args.mode}")
    print()
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not test_connection():
        print("\nè¯·å…ˆè§£å†³æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œç„¶åé‡æ–°è¿è¡Œè„šæœ¬ã€‚")
        return
    
    # å»ºç«‹ SQLAlchemy è¿æ¥
    connection_str = f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4"
    engine = create_engine(connection_str)
    
    # æµ‹è¯• SQLAlchemy è¿æ¥
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print("âœ… SQLAlchemy è¿æ¥æµ‹è¯•æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ SQLAlchemy è¿æ¥å¤±è´¥: {e}")
        return
    
    # ç¡®å®šè¦å¯¼å…¥çš„æ•°æ®ç±»å‹
    if args.type == 'all':
        types_to_import = ['star', 'fork', 'issue', 'comment']
    else:
        types_to_import = [args.type]
    
    # å¯¼å…¥æ•°æ®
    start_time = datetime.now()
    success_count = 0
    
    for data_type in types_to_import:
        # å¯¹äºç¬¬ä¸€ç§ç±»å‹ä½¿ç”¨æŒ‡å®šçš„æ¨¡å¼ï¼Œåç»­ä½¿ç”¨ appendï¼ˆå¦‚æœæ˜¯ all æ¨¡å¼ï¼‰
        mode = args.mode if data_type == types_to_import[0] else 'append' if args.mode == 'replace' else args.mode
        if import_data_type(engine, data_type, args.mode):
            success_count += 1
    
    # æ±‡æ€»
    elapsed = (datetime.now() - start_time).total_seconds()
    print()
    print("=" * 60)
    print(f"âœ… å¯¼å…¥å®Œæˆï¼")
    print(f"   æˆåŠŸå¯¼å…¥: {success_count}/{len(types_to_import)} ç§æ•°æ®ç±»å‹")
    print(f"   æ€»è€—æ—¶: {elapsed:.1f} ç§’")
    print("=" * 60)

if __name__ == '__main__':
    main()
