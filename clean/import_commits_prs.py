"""
å°† commit_activity å’Œ pr_daily æ•°æ®å¯¼å…¥åˆ° MySQL æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python import_commits_prs.py                    # å¯¼å…¥æ‰€æœ‰ç±»å‹
    python import_commits_prs.py --type commit      # åªå¯¼å…¥ commit æ•°æ®
    python import_commits_prs.py --type pr          # åªå¯¼å…¥ PR æ•°æ®
    python import_commits_prs.py --mode append      # è¿½åŠ æ¨¡å¼

ç›®æ ‡æ•°æ®åº“:
    - Dockerå®¹å™¨: openpulse_data
    - æ•°æ®åº“: openrankdata
    - ç«¯å£: 3306
    - å¯†ç : root
"""

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.types import BigInteger, Text, Integer, Date
import pymysql
import argparse
import os
import sys
import json
import shutil
from datetime import datetime

# ç¦ç”¨è¾“å‡ºç¼“å†²
sys.stdout.reconfigure(line_buffering=True)

# ====== æ•°æ®åº“é…ç½® ======
DB_USER = 'root'
DB_PASSWORD = 'root'
DB_HOST = '127.0.0.1'
DB_PORT = 3306
DB_NAME = 'openrankdata'

# ====== æ•°æ®æ–‡ä»¶å¤¹é…ç½® ======
DATA_BASE_PATH = r'D:\openrankdata\crawls\data'

# æ•°æ®ç±»å‹é…ç½®
DATA_CONFIGS = {
    'commit': {
        'folder': 'commit_activity',
        'table': 'commit_activity',
        'file_suffix': '_commits.json',
        'daily_key': 'daily_commits',
        'count_field': 'commit_count'
    },
    'pr': {
        'folder': 'pr_daily',
        'table': 'pr_daily',
        'file_suffix': '_prs.json',
        'daily_key': 'daily_prs',
        'count_field': 'pr_count'
    }
}

# ç£ç›˜ç©ºé—´é˜ˆå€¼ (GB)
MIN_DISK_SPACE_GB = 10


def get_disk_free_space_gb(path='C:\\'):
    """è·å–ç£ç›˜å‰©ä½™ç©ºé—´ (GB)"""
    total, used, free = shutil.disk_usage(path)
    return free / (1024 ** 3)


def check_disk_space():
    """æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦è¶³å¤Ÿ"""
    free_gb = get_disk_free_space_gb()
    if free_gb < MIN_DISK_SPACE_GB:
        print(f"\nâš ï¸ è­¦å‘Š: Cç›˜å‰©ä½™ç©ºé—´ä¸è¶³ ({free_gb:.1f}GB < {MIN_DISK_SPACE_GB}GB)")
        print("â¸ï¸ å¯¼å…¥å·²æš‚åœï¼Œè¯·æ¸…ç†ç£ç›˜ç©ºé—´åé‡æ–°è¿è¡Œ")
        return False
    return True


def test_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥å¹¶åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    try:
        test_conn = pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            charset='utf8mb4'
        )
        print("âœ… MySQL æœåŠ¡å™¨è¿æ¥æˆåŠŸï¼")
        
        cursor = test_conn.cursor()
        cursor.execute(f"CREATE DATABASE IF NOT EXISTS {DB_NAME} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci")
        cursor.execute(f"SHOW DATABASES LIKE '{DB_NAME}'")
        if cursor.fetchone():
            print(f"âœ… æ•°æ®åº“ '{DB_NAME}' å·²å­˜åœ¨")
        
        cursor.close()
        test_conn.close()
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False


def parse_json_file(file_path, config):
    """è§£æ JSON æ–‡ä»¶å¹¶è½¬æ¢ä¸º DataFrame"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    project = data.get('project', '')
    daily_data = data.get(config['daily_key'], {})
    
    # è½¬æ¢ä¸ºè®°å½•åˆ—è¡¨
    records = []
    for date_str, count in daily_data.items():
        records.append({
            'project': project,
            'date': date_str,
            config['count_field']: count
        })
    
    return records


def import_data_type(engine, data_type, import_mode):
    """å¯¼å…¥æŒ‡å®šç±»å‹çš„æ•°æ®"""
    config = DATA_CONFIGS[data_type]
    folder_path = os.path.join(DATA_BASE_PATH, config['folder'])
    
    if not os.path.exists(folder_path):
        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        return False
    
    # è·å–æ‰€æœ‰ JSON æ–‡ä»¶
    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
    total_files = len(json_files)
    
    if total_files == 0:
        print(f"âš ï¸ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰ JSON æ–‡ä»¶: {folder_path}")
        return False
    
    print(f"\nğŸ“‚ æ­£åœ¨å¯¼å…¥ {data_type} æ•°æ®...")
    print(f"   æ–‡ä»¶å¤¹: {folder_path}")
    print(f"   æ–‡ä»¶æ•°: {total_files}")
    print(f"   ç›®æ ‡è¡¨: {config['table']}")
    print(f"   å¯¼å…¥æ¨¡å¼: {import_mode}")
    
    all_records = []
    start_time = datetime.now()
    
    for idx, filename in enumerate(json_files, 1):
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        if idx % 10 == 0 and not check_disk_space():
            return False
        
        file_path = os.path.join(folder_path, filename)
        try:
            records = parse_json_file(file_path, config)
            all_records.extend(records)
            
            free_gb = get_disk_free_space_gb()
            progress = (idx / total_files) * 100
            print(f"   [{idx}/{total_files}] {progress:.1f}% | {filename} | {len(records)} è¡Œ | Cç›˜: {free_gb:.1f}GB")
            
        except Exception as e:
            print(f"   âŒ å¤„ç† {filename} å¤±è´¥: {e}")
            continue
    
    if not all_records:
        print(f"âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å…¥")
        return False
    
    # è½¬æ¢ä¸º DataFrame
    print(f"\n   ğŸ“Š åˆå¹¶ {len(json_files)} ä¸ªæ–‡ä»¶çš„æ•°æ®...")
    df = pd.DataFrame(all_records)
    
    # å®šä¹‰æ•°æ®ç±»å‹
    dtype_mapping = {
        'project': Text(),
        'date': Text(),
        config['count_field']: Integer()
    }
    
    # å†™å…¥æ•°æ®åº“
    print(f"   ğŸ“Š å†™å…¥æ•°æ®åº“ ({len(df)} è¡Œ)...")
    df.to_sql(
        name=config['table'],
        con=engine,
        if_exists=import_mode,
        index=False,
        dtype=dtype_mapping
    )
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"   âœ… {data_type} å¯¼å…¥å®Œæˆï¼")
    print(f"      æ–‡ä»¶æ•°: {len(json_files)}")
    print(f"      æ€»è¡Œæ•°: {len(df)}")
    print(f"      è€—æ—¶: {elapsed:.1f} ç§’")
    
    return True


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å¯¼å…¥ commit_activity å’Œ pr_daily æ•°æ®åˆ° MySQL')
    parser.add_argument('--type', choices=['commit', 'pr', 'all'],
                       default='all',
                       help='æ•°æ®ç±»å‹ï¼šcommit/pr/all (é»˜è®¤: all)')
    parser.add_argument('--mode', choices=['replace', 'append', 'fail'],
                       default='replace',
                       help='å¯¼å…¥æ¨¡å¼ï¼šreplace/append/fail (é»˜è®¤: replace)')
    return parser.parse_args()


def main():
    args = parse_args()
    
    print("=" * 60)
    print("ğŸ“¦ Commit/PR æ•°æ®å¯¼å…¥å·¥å…·")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print()
    
    # æ˜¾ç¤ºç£ç›˜ç©ºé—´
    free_gb = get_disk_free_space_gb()
    print(f"ğŸ’¾ Cç›˜å‰©ä½™ç©ºé—´: {free_gb:.2f} GB")
    print(f"ğŸ’¾ æœ€å°ç©ºé—´é˜ˆå€¼: {MIN_DISK_SPACE_GB} GB")
    print(f"ğŸ“‹ æ•°æ®ç±»å‹: {args.type}")
    print(f"ğŸ“‹ å¯¼å…¥æ¨¡å¼: {args.mode}")
    print()
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    if not check_disk_space():
        return
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not test_connection():
        print("\nè¯·å…ˆè§£å†³æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œç„¶åé‡æ–°è¿è¡Œè„šæœ¬ã€‚")
        return
    
    # å»ºç«‹ SQLAlchemy è¿æ¥
    connection_str = f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4"
    engine = create_engine(connection_str)
    
    # æµ‹è¯•è¿æ¥
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print("âœ… SQLAlchemy è¿æ¥æµ‹è¯•æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ SQLAlchemy è¿æ¥å¤±è´¥: {e}")
        return
    
    # ç¡®å®šè¦å¯¼å…¥çš„æ•°æ®ç±»å‹
    if args.type == 'all':
        data_types = ['commit', 'pr']
    else:
        data_types = [args.type]
    
    # å¯¼å…¥æ•°æ®
    total_start = datetime.now()
    success_count = 0
    
    for data_type in data_types:
        if import_data_type(engine, data_type, args.mode):
            success_count += 1
    
    # æ€»ç»“
    total_elapsed = (datetime.now() - total_start).total_seconds()
    print()
    print("=" * 60)
    print("ğŸ“Š å¯¼å…¥å®Œæˆ!")
    print(f"   æˆåŠŸ: {success_count}/{len(data_types)} ä¸ªæ•°æ®ç±»å‹")
    print(f"   æ€»è€—æ—¶: {total_elapsed:.1f} ç§’")
    print("=" * 60)


if __name__ == '__main__':
    main()
