"""
Commit Activity å’Œ PR æ•°é‡çˆ¬è™«è„šæœ¬
ä½¿ç”¨GitHub APIçˆ¬å–top300é¡¹ç›®æ¯å¤©çš„commitæ•°é‡å’ŒPRæ•°é‡

åŠŸèƒ½:
- æ–­ç‚¹ç»­ä¼ æ”¯æŒ
- å¤šTokenè½®æ¢
- è·å–æ¯ä¸ªé¡¹ç›®çš„æ¯æ—¥commitæ•°é‡
- è·å–æ¯ä¸ªé¡¹ç›®çš„æ¯æ—¥PRåˆ›å»ºæ•°é‡
- æ•°æ®å­˜å‚¨åˆ° data/commit_activity/ å’Œ data/pr_daily/ ç›®å½•

å‚è€ƒ: crawls/crawl_stars.py çš„å­˜å‚¨è·¯å¾„ç»“æ„
"""

import os
import json
import time
import sys
from datetime import datetime, timezone, timedelta
from collections import defaultdict
import requests
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()

TOKENS = [
    os.getenv("GITHUB_TOKEN_1", "your_github_token_1"),
    os.getenv("GITHUB_TOKEN_2", "your_github_token_2"),
    os.getenv("GITHUB_TOKEN_3", "your_github_token_3"),
    os.getenv("GITHUB_TOKEN_4", "your_github_token_4"),
]
PROJECT_LIST_FILE = "top300_projects_list.txt"
DATA_DIR = "data"
COMMIT_DIR = os.path.join(DATA_DIR, "commit_activity")
PR_DIR = os.path.join(DATA_DIR, "pr_daily")
CHECKPOINT_DIR = os.path.join(DATA_DIR, "commits_prs_checkpoint")

START_DATE = datetime(2022, 3, 1, tzinfo=timezone.utc)
END_DATE = datetime(2023, 3, 31, 23, 59, 59, tzinfo=timezone.utc)


class GitHubCrawler:
    def __init__(self, tokens):
        self.tokens = tokens
        self.current_token_index = 0
        self.session = requests.Session()
        self._update_headers()
    
    def _update_headers(self):
        token = self.tokens[self.current_token_index % len(self.tokens)]
        self.session.headers.update({
            'Authorization': f'Bearer {token}',
            'Accept': 'application/vnd.github.v3+json',
            'X-GitHub-Api-Version': '2022-11-28'
        })
    
    def switch_token(self):
        self.current_token_index += 1
        self._update_headers()
        print(f"åˆ‡æ¢åˆ°Token {self.current_token_index % len(self.tokens) + 1}")
        return self.current_token_index
    
    def get_rate_limit_info(self):
        url = "https://api.github.com/rate_limit"
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                core = data.get('resources', {}).get('core', {})
                search = data.get('resources', {}).get('search', {})
                return {
                    'core_remaining': core.get('remaining', 0),
                    'core_reset': core.get('reset', 0),
                    'search_remaining': search.get('remaining', 0),
                    'search_reset': search.get('reset', 0)
                }
        except:
            pass
        return {'core_remaining': 0, 'core_reset': 0, 'search_remaining': 0, 'search_reset': 0}
    
    def get_with_retry(self, url, params=None, max_retries=3, is_search=False):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, params=params, timeout=30)
                
                remaining = int(response.headers.get('X-RateLimit-Remaining', 1))
                if remaining == 0:
                    reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
                    wait_time = max(reset_time - time.time(), 0) + 5
                    print(f"\nRate limitè¾¾åˆ°ï¼Œåˆ‡æ¢token...")
                    old_index = self.current_token_index
                    self.switch_token()
                    if self.current_token_index >= old_index + len(self.tokens):
                        actual_wait = min(wait_time, 60) if is_search else min(wait_time, 30)
                        print(f"æ‰€æœ‰tokenéƒ½è¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾… {actual_wait:.0f} ç§’...")
                        time.sleep(actual_wait)
                    continue
                
                if response.status_code == 200:
                    return response.json(), response.headers
                elif response.status_code == 202:
                    print(f"\næ•°æ®ç”Ÿæˆä¸­ï¼Œç­‰å¾…2ç§’åé‡è¯•...")
                    time.sleep(2)
                    continue
                elif response.status_code == 403:
                    error_msg = response.json().get('message', '')
                    if 'rate limit' in error_msg.lower() or 'secondary rate limit' in error_msg.lower():
                        print(f"\n403 Rate limit, åˆ‡æ¢token...")
                        self.switch_token()
                        time.sleep(2)
                        continue
                    else:
                        print(f"\n403 Forbidden: {error_msg}")
                        return None, None
                elif response.status_code == 404:
                    return None, None
                elif response.status_code == 422:
                    print(f"\n422 Unprocessable: {response.text[:200]}")
                    return None, None
                elif response.status_code == 409:
                    print(f"\n409 Conflict (å¯èƒ½æ˜¯ç©ºä»“åº“)")
                    return None, None
                else:
                    print(f"\nHTTP {response.status_code}: {response.text[:200]}")
                    time.sleep(2)
                    
            except requests.exceptions.RequestException as e:
                print(f"\nè¯·æ±‚é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                time.sleep(5)
        
        return None, None

    def get_commits_page(self, owner, repo, since=None, until=None, page=1, per_page=100):
        url = f"https://api.github.com/repos/{owner}/{repo}/commits"
        params = {'page': page, 'per_page': per_page}
        if since:
            params['since'] = since.isoformat()
        if until:
            params['until'] = until.isoformat()
        data, headers = self.get_with_retry(url, params)
        return data, headers
    
    def get_prs_page(self, owner, repo, state='all', page=1, per_page=100):
        url = f"https://api.github.com/repos/{owner}/{repo}/pulls"
        params = {
            'state': state,
            'sort': 'created',
            'direction': 'desc',
            'page': page, 
            'per_page': per_page
        }
        data, headers = self.get_with_retry(url, params)
        return data, headers
    
    def search_prs(self, repo, created_date):
        url = "https://api.github.com/search/issues"
        query = f"repo:{repo} is:pr created:{created_date}"
        params = {'q': query, 'per_page': 1}
        data, headers = self.get_with_retry(url, params, is_search=True)
        if data:
            return data.get('total_count', 0)
        return 0


def ensure_dirs():
    for d in [COMMIT_DIR, PR_DIR, CHECKPOINT_DIR]:
        if not os.path.exists(d):
            os.makedirs(d)


def get_projects():
    projects = []
    if not os.path.exists(PROJECT_LIST_FILE):
        print(f"Error: {PROJECT_LIST_FILE} not found.")
        return []
        
    with open(PROJECT_LIST_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: 
                continue
            if 'â†’' in line:
                projects.append(line.split('â†’')[-1].strip())
            else:
                projects.append(line)
    return projects


def get_safe_name(repo_name):
    return repo_name.replace('/', '_')


def get_checkpoint_path(repo_name, data_type):
    safe_name = get_safe_name(repo_name)
    return os.path.join(CHECKPOINT_DIR, f"{safe_name}_{data_type}.json")


def get_output_path(repo_name, data_type):
    safe_name = get_safe_name(repo_name)
    if data_type == 'commits':
        return os.path.join(COMMIT_DIR, f"{safe_name}_commits.json")
    else:
        return os.path.join(PR_DIR, f"{safe_name}_prs.json")


def read_checkpoint(repo_name, data_type):
    path = get_checkpoint_path(repo_name, data_type)
    if os.path.exists(path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {"last_page": 0, "daily_counts": {}, "completed": False}


def write_checkpoint(repo_name, data_type, checkpoint_data):
    path = get_checkpoint_path(repo_name, data_type)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)


def save_result(repo_name, data_type, daily_counts, total_count):
    path = get_output_path(repo_name, data_type)
    
    sorted_dates = sorted(daily_counts.keys())
    
    result = {
        "project": repo_name,
        "data_type": data_type,
        f"total_{data_type}_in_range": sum(daily_counts.values()),
        f"total_{data_type}_all_time": total_count,
        "start_date": START_DATE.strftime("%Y-%m-%d"),
        "end_date": END_DATE.strftime("%Y-%m-%d"),
        "crawled_at": datetime.now(timezone.utc).isoformat(),
        f"daily_{data_type}": {date: daily_counts[date] for date in sorted_dates}
    }
    
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def process_commits(crawler, repo_name):
    parts = repo_name.split('/')
    if len(parts) != 2:
        print(f"âš ï¸  è·³è¿‡æ— æ•ˆé¡¹ç›®æ ¼å¼: {repo_name}")
        return False
    
    owner, repo = parts
    
    checkpoint = read_checkpoint(repo_name, 'commits')
    
    if checkpoint.get("completed", False):
        print(f"  [Commits] å·²å®Œæˆï¼Œè·³è¿‡")
        return True
    
    last_page = checkpoint.get("last_page", 0)
    daily_commits = defaultdict(int, checkpoint.get("daily_counts", {}))
    
    print(f"  [Commits] å¼€å§‹çˆ¬å–ï¼Œä»ç¬¬ {last_page + 1} é¡µç»§ç»­...")
    
    page = last_page + 1
    total_commits = 0
    commits_in_range = 0
    
    pbar = tqdm(desc=f"    Commits", unit=" pages", initial=last_page, leave=False)
    
    try:
        while True:
            data, headers = crawler.get_commits_page(
                owner, repo, 
                since=START_DATE, 
                until=END_DATE,
                page=page, 
                per_page=100
            )
            
            if data is None or len(data) == 0:
                break
            
            for commit_info in data:
                total_commits += 1
                
                commit_data = commit_info.get('commit', {})
                committer = commit_data.get('committer', {})
                commit_date_str = committer.get('date')
                
                if not commit_date_str:
                    author = commit_data.get('author', {})
                    commit_date_str = author.get('date')
                
                if not commit_date_str:
                    continue
                
                try:
                    commit_date = datetime.fromisoformat(commit_date_str.replace('Z', '+00:00'))
                except:
                    continue
                
                if START_DATE <= commit_date <= END_DATE:
                    date_str = commit_date.strftime("%Y-%m-%d")
                    daily_commits[date_str] += 1
                    commits_in_range += 1
            
            pbar.update(1)
            
            link_header = headers.get('Link', '') if headers else ''
            if 'rel="next"' not in link_header:
                break
            
            if page % 10 == 0:
                checkpoint_data = {
                    "last_page": page,
                    "daily_counts": dict(daily_commits),
                    "completed": False
                }
                write_checkpoint(repo_name, 'commits', checkpoint_data)
            
            page += 1
            
            time.sleep(0.1)
        
        pbar.close()
        
        save_result(repo_name, 'commits', dict(daily_commits), total_commits)
        
        checkpoint_data = {
            "last_page": page,
            "daily_counts": dict(daily_commits),
            "completed": True
        }
        write_checkpoint(repo_name, 'commits', checkpoint_data)
        
        print(f"  [Commits] å®Œæˆ! æ€»æ•°: {total_commits}, èŒƒå›´å†…: {commits_in_range}")
        return True
        
    except KeyboardInterrupt:
        print(f"\n  [Commits] ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...")
        checkpoint_data = {
            "last_page": page - 1,
            "daily_counts": dict(daily_commits),
            "completed": False
        }
        write_checkpoint(repo_name, 'commits', checkpoint_data)
        raise
    except Exception as e:
        print(f"\n  [Commits] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        checkpoint_data = {
            "last_page": page - 1,
            "daily_counts": dict(daily_commits),
            "completed": False
        }
        write_checkpoint(repo_name, 'commits', checkpoint_data)
        return False


def process_prs(crawler, repo_name):
    parts = repo_name.split('/')
    if len(parts) != 2:
        print(f"âš ï¸  è·³è¿‡æ— æ•ˆé¡¹ç›®æ ¼å¼: {repo_name}")
        return False
    
    owner, repo = parts
    
    checkpoint = read_checkpoint(repo_name, 'prs')
    
    if checkpoint.get("completed", False):
        print(f"  [PRs] å·²å®Œæˆï¼Œè·³è¿‡")
        return True
    
    last_page = checkpoint.get("last_page", 0)
    daily_prs = defaultdict(int, checkpoint.get("daily_counts", {}))
    
    print(f"  [PRs] å¼€å§‹çˆ¬å–ï¼Œä»ç¬¬ {last_page + 1} é¡µç»§ç»­...")
    
    page = last_page + 1
    total_prs = 0
    prs_in_range = 0
    passed_start_date = False 
    
    pbar = tqdm(desc=f"    PRs", unit=" pages", initial=last_page, leave=False)
    
    try:
        while True:
            data, headers = crawler.get_prs_page(owner, repo, state='all', page=page, per_page=100)
            
            if data is None or len(data) == 0:
                break
            
            for pr_info in data:
                total_prs += 1
                
                created_at_str = pr_info.get('created_at')
                if not created_at_str:
                    continue
                
                try:
                    created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
                except:
                    continue
                
                if START_DATE <= created_at <= END_DATE:
                    date_str = created_at.strftime("%Y-%m-%d")
                    daily_prs[date_str] += 1
                    prs_in_range += 1
                
                if created_at < START_DATE:
                    passed_start_date = True
            
            pbar.update(1)
            
            if passed_start_date:
                all_before = all(
                    datetime.fromisoformat(pr.get('created_at', '').replace('Z', '+00:00')) < START_DATE
                    for pr in data if pr.get('created_at')
                )
                if all_before:
                    break
            
            link_header = headers.get('Link', '') if headers else ''
            if 'rel="next"' not in link_header:
                break
            
            if page % 10 == 0:
                checkpoint_data = {
                    "last_page": page,
                    "daily_counts": dict(daily_prs),
                    "completed": False
                }
                write_checkpoint(repo_name, 'prs', checkpoint_data)
            
            page += 1
            
            time.sleep(0.1)
        
        pbar.close()
        
        save_result(repo_name, 'prs', dict(daily_prs), total_prs)
        
        checkpoint_data = {
            "last_page": page,
            "daily_counts": dict(daily_prs),
            "completed": True
        }
        write_checkpoint(repo_name, 'prs', checkpoint_data)
        
        print(f"  [PRs] å®Œæˆ! æ€»æ•°: {total_prs}, èŒƒå›´å†…: {prs_in_range}")
        return True
        
    except KeyboardInterrupt:
        print(f"\n  [PRs] ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...")
        checkpoint_data = {
            "last_page": page - 1,
            "daily_counts": dict(daily_prs),
            "completed": False
        }
        write_checkpoint(repo_name, 'prs', checkpoint_data)
        raise
    except Exception as e:
        print(f"\n  [PRs] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        checkpoint_data = {
            "last_page": page - 1,
            "daily_counts": dict(daily_prs),
            "completed": False
        }
        write_checkpoint(repo_name, 'prs', checkpoint_data)
        return False


def process_repo(crawler, repo_name):
    commit_success = process_commits(crawler, repo_name)
    pr_success = process_prs(crawler, repo_name)
    return commit_success and pr_success


def main():
    print("=" * 60)
    print("ğŸ“Š GitHub Commit Activity & PR æ•°æ®çˆ¬è™« (æ¯æ—¥ç»Ÿè®¡)")
    print("=" * 60)
    print(f"\nğŸ“ é¡¹ç›®åˆ—è¡¨: {PROJECT_LIST_FILE}")
    print(f"ğŸ“ Commitæ•°æ®ç›®å½•: {COMMIT_DIR}")
    print(f"ğŸ“ PRæ•°æ®ç›®å½•: {PR_DIR}")
    print(f"ğŸ“ æ–­ç‚¹ç›®å½•: {CHECKPOINT_DIR}")
    print(f"ğŸ”‘ Tokenæ•°é‡: {len(TOKENS)}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    
    ensure_dirs()
    
    if len(sys.argv) > 1:
        projects = [sys.argv[1]]
    else:
        projects = get_projects()
    
    if not projects:
        print("âŒ æœªæ‰¾åˆ°é¡¹ç›®åˆ—è¡¨")
        return
    
    print(f"\nğŸ“‹ æ‰¾åˆ° {len(projects)} ä¸ªé¡¹ç›®")
    
    crawler = GitHubCrawler(TOKENS)
    
    rate_info = crawler.get_rate_limit_info()
    print(f"ğŸ“Š å½“å‰Tokenå‰©ä½™è¯·æ±‚æ¬¡æ•°: Core={rate_info['core_remaining']}, Search={rate_info['search_remaining']}")
    
    print(f"\nğŸš€ å¼€å§‹çˆ¬å–...\n")
    
    success_count = 0
    error_count = 0
    skipped_count = 0
    
    for i, repo_name in enumerate(projects):
        print(f"\n[{i+1}/{len(projects)}] å¤„ç†: {repo_name}")
        
        commit_checkpoint = read_checkpoint(repo_name, 'commits')
        pr_checkpoint = read_checkpoint(repo_name, 'prs')
        if commit_checkpoint.get("completed", False) and pr_checkpoint.get("completed", False):
            print(f"  âœ“ å·²å®Œæˆï¼Œè·³è¿‡")
            skipped_count += 1
            continue
        
        try:
            success = process_repo(crawler, repo_name)
            if success:
                success_count += 1
            else:
                error_count += 1
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­!")
            break
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            error_count += 1
            crawler.switch_token()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
    print("=" * 60)
    print(f"æ€»é¡¹ç›®æ•°: {len(projects)}")
    print(f"æˆåŠŸ: {success_count}")
    print(f"è·³è¿‡(å·²å®Œæˆ): {skipped_count}")
    print(f"å¤±è´¥: {error_count}")
    print("\nâœ… çˆ¬å–å®Œæˆ!")

if __name__ == "__main__":
    main()
